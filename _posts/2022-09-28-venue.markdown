---
layout: post
title:  "ManiSkill2: A Unified Benchmark for Generalizable Manipulation Skills"
image: /images/maniskill2_teaser.png
categories: research
authors: "Jiayuan Gu†, Fanbo Xiang†, <strong>Xuanlin Li*</strong>, Zhan Ling*, Xiqiang Liu*, Tongzhou Mu*, Yihe Tang*, Stone Tao*, Xinyue Wei*, Yunchao Yao*, Xiaodi Yuan, Pengwei Xie, Zhiao Huang, Rui Chen, Hao Su"
venue: ICLR 2023
arxiv: https://arxiv.org/abs/2302.04659
website: https://maniskill2.github.io
code: https://github.com/haosulab/ManiSkill2
implementation: https://github.com/haosulab/ManiSkill2-Learn
---
We present ManiSkill2, the next generation of the SAPIEN ManiSkill benchmark, to address critical pain points often encountered by researchers when using benchmarks for generalizable manipulation skills. ManiSkill2 includes 20 manipulation task families with 2000+ object models and 4M+ demonstration frames, which cover stationary/mobile-base, single/dual-arm, and rigid/soft-body manipulation tasks with 2D/3D-input data simulated by fully dynamic engines. It defines a unified interface and evaluation protocol to support a wide range of algorithms (e.g., classic sense-plan-act, RL, IL), visual observations (point cloud, RGBD), and controllers (e.g., action type and parameterization). Moreover, it empowers fast visual input learning algorithms so that a CNN-based policy can collect samples at about 2000 FPS with 1 GPU and 16 processes on a regular workstation. 