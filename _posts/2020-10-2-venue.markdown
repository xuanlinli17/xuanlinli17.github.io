---
layout: post
title:  "Discovering Non-Monotonic Autoregressive Orderings with Variational Inference"
image: /images/voi.png
categories: research
authors: "<strong>Xuanlin Li*</strong>, Brandon Trabucco*, Dong Huk Park, Yang Gao, Michael Luo, Sheng Shen, Trevor Darrell"
venue: <em>International Conference on Learning Representations (ICLR) 2021</em>
paper: https://arxiv.org/abs/2110.15797
code: https://github.com/xuanlinli17/autoregressive_inference
slides: https://docs.google.com/presentation/d/1w5D9TrOKKMnT9ip81LnbtxfW-4UHIVc5sOd2W4phGiw/edit?usp=sharing
poster: https://drive.google.com/file/d/1rZooC6ED4MemoL2Sfo-xNPeZwkSFDyR8/view?usp=sharing
video_transcripts: https://docs.google.com/document/d/1KxBJBNev4UzYcybqRwGz27gtmH_LG2pQhGFTJXSUZzg/edit?usp=sharing
---
We propose the first domain-independent unsupervised / self-supervised learner that discovers high-quality autoregressive orders through fully-parallelizable end-to-end training in a data-driven manner - no domain knowledge required. The learner contains an encoder network and decoder language model that perform variational inference with autoregressive orders (represented as permutation matrices) as latent variables. The corresponding ELBO is not differentiable, so we develop a practical algorithm for end-to-end optimization using policy gradients. We implement the encoder as a Transformer with non-causal attention that outputs permutations in one forward pass. Permutations then serve as target generation orders for training an insertion-based Transformer language model. Empirical results in language modeling tasks demonstrate that our method is context-aware and discovers orderings that are competitive with or even better than fixed orders.
